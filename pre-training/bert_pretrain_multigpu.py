from transformers import BertForMaskedLM, BertTokenizer,BertConfig,Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset,Dataset,concatenate_datasets
import torch
import json
import multiprocessing
from tqdm import tqdm
from datetime import datetime

# å®šä¹‰ tokenize å‡½æ•°
def tokenize_function(examples, tokenizer):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# å¤šè¿›ç¨‹å¤„ç†å‡½æ•°
def process_batch(args):
    batch, tokenizer = args
    return Dataset.from_dict(tokenize_function(batch, tokenizer))

# ä¸»å‡½æ•°
def main(dataset, tokenizer, batch_size=1000, num_proc=6):
    # å°†æ•°æ®é›†åˆ†å—
    batches = [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]

    # åˆ›å»ºè¿›ç¨‹æ± 
    pool = multiprocessing.Pool(processes=num_proc)

    # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•°ï¼ˆå°† tokenizer ä¼ é€’ç»™æ¯ä¸ªè¿›ç¨‹ï¼‰
    task_args = [(batch, tokenizer) for batch in batches]

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    tokenized_batches = []
    with tqdm(total=len(batches), desc="Tokenizing") as pbar:
        for result in pool.imap(process_batch, task_args):
            tokenized_batches.append(result)
            pbar.update(1)

    # å…³é—­è¿›ç¨‹æ± 
    pool.close()
    pool.join()

    # åˆå¹¶å¤„ç†åçš„ç»“æœ
    tokenized_datasets = concatenate_datasets(tokenized_batches)
    return tokenized_datasets

if __name__ == "__main__":
    #  æ¨¡å‹ï¼Œåˆ†è¯å™¨æ··åˆæ•°æ®è·¯å¾„
    model_path="model\\bert\\bert_base"
    tokenizer_path="vocab\\bert_tokenizer"
    dataset_path="bert_pretrain_payloads.json"

    # **1ï¸âƒ£ åŠ è½½ BERT MLM é¢„è®­ç»ƒæ¨¡å‹**
    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
    vocab_size=tokenizer.vocab_size

    #åŠ è½½æ¨¡å‹configå¹¶å¢åŠ éšè—å±‚å±æ€§ï¼Œæ–¹ä¾¿åç»­æå–ä»¥åŠä¿®æ”¹è¯åº“å¤§å°,ç¬¬ä¸€æ¬¡è®­ç»ƒä¿®æ”¹åå°±å¯ä»¥ä¸æ·»åŠ äº†
    config = BertConfig.from_pretrained(model_path)
    config.output_hidden_states = True

     # ä¿®æ”¹ä¸ºæ–°çš„è¯åº“å¤§å°
    config.vocab_size = vocab_size
    config.save_pretrained(model_path)

    # åŠ è½½ç©ºæ¨¡å‹
    model = BertForMaskedLM.from_pretrained(model_path,config=config, ignore_mismatched_sizes=True)

    # **2ï¸âƒ£ åŠ è½½æ•°æ®é›†**
    #è¯»å–æ•°æ®
    with open(dataset_path, "r") as file:
        payloads = json.load(file)
    data = []
    for payload in payloads:
        flow = ""
        for j in range(3):
            flow += payload
        data.append(flow)

    dataset = Dataset.from_dict({"text": data})

    # **3ï¸âƒ£ å®šä¹‰æ•°æ®é¢„å¤„ç†**

    # è°ƒç”¨ä¸»å‡½æ•°
    tokenized_datasets = main(dataset, tokenizer, batch_size=1000, num_proc=30)
    # tokenized_datasets = tokenize_function(dataset)

    # **4ï¸âƒ£ åˆ›å»º MLM ä»»åŠ¡çš„æ•°æ®åŠ è½½å™¨**
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=0.15  # Mask 15% çš„ token
    )

    # **5ï¸âƒ£ è®­ç»ƒå‚æ•°**

    training_args = TrainingArguments(
        output_dir="C:\\Users\\Administrator\\bertgnn\\model\\bert\\pretrained_tinybert",
        # evaluation_strategy="epoch",      # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
        save_strategy="epoch",            # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
        logging_steps=500,
        save_total_limit=2,
        per_device_train_batch_size=64,   # ğŸš€ æ¯ä¸ª GPU çš„ batch size
        # per_device_eval_batch_size=16,
        gradient_accumulation_steps=2,    # ğŸš€ ç´¯ç§¯æ¢¯åº¦ï¼Œç›¸å½“äº batch size = 128
        num_train_epochs=10,
        learning_rate=5e-5,
        warmup_steps=1000,
        weight_decay=0.01,
        fp16=True,                        # ğŸš€ å¼€å¯æ··åˆç²¾åº¦
        dataloader_num_workers=4,
        ddp_find_unused_parameters=False, # ğŸš€ é€‚ç”¨äº DDP è®­ç»ƒï¼Œé¿å…æŠ¥é”™
        logging_dir="C:\\Users\\Administrator\\bertgnn\\model\\bert\\pretrained_mlmbert\\logs",
        report_to="none"
    )

    # **6ï¸âƒ£ åˆ›å»º Trainer**
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets,
        # eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator
    )


    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()

    # **7ï¸âƒ£ è¿è¡Œè®­ç»ƒ**
    trainer.train()

    # è®°å½•ç»“æŸæ—¶é—´
    end_time = datetime.now()

    # è®¡ç®—è¿è¡Œæ—¶é—´
    elapsed_time = end_time - start_time
    print(f"ä»£ç è¿è¡Œæ—¶é—´: {elapsed_time}")
